From f32f1a4cfb19bc6187a852fb07f3a8f990f6650d Mon Sep 17 00:00:00 2001
From: Min Xu <24926999+min-xu-ai@users.noreply.github.com>
Date: Fri, 23 Sep 2022 18:07:02 -0700
Subject: [PATCH 1/2] [chore] move fair_dev into fairscale (#1078)

Co-authored-by: Min Xu <min.xu.public@gmail.com>
---
 benchmarks/experimental/experimental_async_approaches.py  | 2 +-
 benchmarks/pipe.py                                        | 2 +-
 fairscale/README.md                                       | 4 ++++
 fairscale/__init__.py                                     | 4 ++++
 {fair_dev => fairscale/fair_dev}/common_paths.py          | 0
 .../fair_dev}/testing/golden_testing_data.py              | 0
 {fair_dev => fairscale/fair_dev}/testing/testing.py       | 0
 .../fair_dev}/testing/testing_memory.py                   | 0
 .../nn/ampnet_pipe_process/test_ampnet_pipe.py            | 2 +-
 tests/experimental/nn/data_parallel/test_gossip.py        | 2 +-
 tests/experimental/nn/test_mevo.py                        | 2 +-
 tests/experimental/nn/test_multiprocess_pipe.py           | 2 +-
 tests/experimental/nn/test_offload.py                     | 2 +-
 tests/experimental/tooling/test_layer_memory_tracker.py   | 2 +-
 tests/experimental/wgit/test_sha1_store.py                | 2 +-
 tests/experimental/wgit/test_signal_sparsity.py           | 2 +-
 tests/experimental/wgit/test_signal_sparsity_profiling.py | 2 +-
 tests/nn/checkpoint/test_checkpoint_activations.py        | 2 +-
 tests/nn/checkpoint/test_checkpoint_activations_norm.py   | 2 +-
 tests/nn/data_parallel/test_fsdp.py                       | 2 +-
 tests/nn/data_parallel/test_fsdp_freezing_weights.py      | 2 +-
 tests/nn/data_parallel/test_fsdp_grad_acc.py              | 2 +-
 tests/nn/data_parallel/test_fsdp_hf_transformer_eval.py   | 2 +-
 tests/nn/data_parallel/test_fsdp_input.py                 | 2 +-
 tests/nn/data_parallel/test_fsdp_memory.py                | 2 +-
 tests/nn/data_parallel/test_fsdp_metadata.py              | 2 +-
 tests/nn/data_parallel/test_fsdp_multiple_forward.py      | 2 +-
 .../test_fsdp_multiple_forward_checkpoint.py              | 2 +-
 tests/nn/data_parallel/test_fsdp_multiple_wrapping.py     | 2 +-
 tests/nn/data_parallel/test_fsdp_offload.py               | 2 +-
 tests/nn/data_parallel/test_fsdp_optimizer_utils.py       | 2 +-
 tests/nn/data_parallel/test_fsdp_overlap.py               | 8 +++++++-
 tests/nn/data_parallel/test_fsdp_pre_backward_hook.py     | 2 +-
 tests/nn/data_parallel/test_fsdp_regnet.py                | 2 +-
 tests/nn/data_parallel/test_fsdp_shared_weights.py        | 8 +++++++-
 tests/nn/data_parallel/test_fsdp_shared_weights_mevo.py   | 4 ++--
 tests/nn/data_parallel/test_fsdp_state_dict.py            | 2 +-
 tests/nn/data_parallel/test_fsdp_uneven.py                | 2 +-
 .../nn/data_parallel/test_fsdp_with_checkpoint_wrapper.py | 2 +-
 tests/nn/data_parallel/test_sharded_ddp_features.py       | 2 +-
 tests/nn/data_parallel/test_sharded_ddp_pytorch_parity.py | 7 ++++++-
 tests/nn/misc/test_flatten_params_wrapper.py              | 2 +-
 tests/nn/model_parallel/test_cross_entropy.py             | 2 +-
 tests/nn/model_parallel/test_initialize.py                | 2 +-
 tests/nn/model_parallel/test_layers.py                    | 2 +-
 tests/nn/model_parallel/test_random.py                    | 2 +-
 tests/nn/moe/test_moe_layer.py                            | 2 +-
 tests/nn/pipe/skip/test_gpipe.py                          | 2 +-
 tests/nn/pipe/test_bugs.py                                | 2 +-
 tests/nn/pipe/test_checkpoint_ddp.py                      | 2 +-
 tests/nn/pipe/test_parity.py                              | 2 +-
 tests/nn/pipe_process/test_bugs.py                        | 2 +-
 tests/nn/pipe_process/test_inplace.py                     | 2 +-
 tests/nn/pipe_process/test_pipe.py                        | 2 +-
 tests/nn/pipe_process/test_rpc.py                         | 2 +-
 tests/nn/pipe_process/test_transparency.py                | 2 +-
 tests/nn/wrap/test_wrap.py                                | 2 +-
 tests/optim/test_ddp_adascale.py                          | 4 ++--
 tests/optim/test_layerwise_gradient_scaler.py             | 4 ++--
 tests/optim/test_oss.py                                   | 2 +-
 tests/optim/test_oss_adascale.py                          | 4 ++--
 tests/optim/test_single_node_adascale.py                  | 6 +++---
 tests/utils/test_reduce_scatter_bucketer.py               | 2 +-
 63 files changed, 88 insertions(+), 63 deletions(-)
 create mode 100644 fairscale/README.md
 rename {fair_dev => fairscale/fair_dev}/common_paths.py (100%)
 rename {fair_dev => fairscale/fair_dev}/testing/golden_testing_data.py (100%)
 rename {fair_dev => fairscale/fair_dev}/testing/testing.py (100%)
 rename {fair_dev => fairscale/fair_dev}/testing/testing_memory.py (100%)

diff --git a/benchmarks/experimental/experimental_async_approaches.py b/benchmarks/experimental/experimental_async_approaches.py
index d4e68af..c6b09e1 100644
--- a/benchmarks/experimental/experimental_async_approaches.py
+++ b/benchmarks/experimental/experimental_async_approaches.py
@@ -21,8 +21,8 @@ from torch.utils.data import DataLoader
 import torchtext
 from torchtext.data.utils import get_tokenizer
 
-from fair_dev.testing.testing import dist_init, get_worker_map
 from fairscale.experimental.nn.ampnet_pipe import pipe
+from fairscale.fair_dev.testing.testing import dist_init, get_worker_map
 from fairscale.nn.model_parallel import initialize_model_parallel
 from fairscale.nn.model_parallel.initialize import get_pipeline_parallel_group
 from fairscale.nn.pipe import LazyModule
diff --git a/benchmarks/pipe.py b/benchmarks/pipe.py
index 9826d8f..595a4eb 100644
--- a/benchmarks/pipe.py
+++ b/benchmarks/pipe.py
@@ -16,7 +16,7 @@ from torch.nn.parallel import DistributedDataParallel as DDP
 import utils
 
 from benchmarks.golden_configs.lm_wikitext2 import Pipe as lm_wikitext2
-from fair_dev.testing.testing import dist_init
+from fairscale.fair_dev.testing.testing import dist_init
 from fairscale.nn import Pipe
 from fairscale.nn.model_parallel import initialize_model_parallel
 
diff --git a/fairscale/README.md b/fairscale/README.md
new file mode 100644
index 0000000..ab60ff6
--- /dev/null
+++ b/fairscale/README.md
@@ -0,0 +1,4 @@
+NOTE:
+
+The experimental and fair_dev submodules are not part of the fairscale public
+API. There can be breaking changes in them at anytime.
diff --git a/fairscale/__init__.py b/fairscale/__init__.py
index e4be89b..033b002 100644
--- a/fairscale/__init__.py
+++ b/fairscale/__init__.py
@@ -5,6 +5,10 @@
 
 ################################################################################
 # Import most common subpackages
+#
+# NOTE: we don't maintain any public APIs in both experimental and fair_dev
+#       sub-modules. Code in them are experimental or for developer only. They
+#       can be changed, removed, anytime.
 ################################################################################
 
 from typing import List
diff --git a/fair_dev/common_paths.py b/fairscale/fair_dev/common_paths.py
similarity index 100%
rename from fair_dev/common_paths.py
rename to fairscale/fair_dev/common_paths.py
diff --git a/fair_dev/testing/golden_testing_data.py b/fairscale/fair_dev/testing/golden_testing_data.py
similarity index 100%
rename from fair_dev/testing/golden_testing_data.py
rename to fairscale/fair_dev/testing/golden_testing_data.py
diff --git a/fair_dev/testing/testing.py b/fairscale/fair_dev/testing/testing.py
similarity index 100%
rename from fair_dev/testing/testing.py
rename to fairscale/fair_dev/testing/testing.py
diff --git a/fair_dev/testing/testing_memory.py b/fairscale/fair_dev/testing/testing_memory.py
similarity index 100%
rename from fair_dev/testing/testing_memory.py
rename to fairscale/fair_dev/testing/testing_memory.py
diff --git a/tests/experimental/nn/ampnet_pipe_process/test_ampnet_pipe.py b/tests/experimental/nn/ampnet_pipe_process/test_ampnet_pipe.py
index 3f24b55..785beee 100644
--- a/tests/experimental/nn/ampnet_pipe_process/test_ampnet_pipe.py
+++ b/tests/experimental/nn/ampnet_pipe_process/test_ampnet_pipe.py
@@ -22,8 +22,8 @@ from torch import nn
 from torch.optim.optimizer import Optimizer
 from torch.utils.data import DataLoader, Dataset
 
-from fair_dev.testing.testing import get_worker_map, torch_spawn
 from fairscale.experimental.nn.ampnet_pipe.pipe import AMPnetPipe
+from fairscale.fair_dev.testing.testing import get_worker_map, torch_spawn
 
 
 class MySGD(Optimizer):
diff --git a/tests/experimental/nn/data_parallel/test_gossip.py b/tests/experimental/nn/data_parallel/test_gossip.py
index bf6de7c..4f725df 100644
--- a/tests/experimental/nn/data_parallel/test_gossip.py
+++ b/tests/experimental/nn/data_parallel/test_gossip.py
@@ -15,8 +15,8 @@ from torch import nn
 import torch.distributed
 import torch.nn.functional as F
 
-from fair_dev.testing.testing import skip_if_single_gpu, spawn_for_all_world_sizes
 import fairscale.experimental.nn.data_parallel.gossip as gossip
+from fairscale.fair_dev.testing.testing import skip_if_single_gpu, spawn_for_all_world_sizes
 
 # Enfore CUBLAS reproducibility, see https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
 os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
diff --git a/tests/experimental/nn/test_mevo.py b/tests/experimental/nn/test_mevo.py
index e8caff6..457f5ac 100644
--- a/tests/experimental/nn/test_mevo.py
+++ b/tests/experimental/nn/test_mevo.py
@@ -12,9 +12,9 @@ import os
 import pytest
 import torch
 
-from fair_dev.testing.testing import skip_if_no_cuda
 from fairscale.experimental.nn import MEVO
 from fairscale.experimental.nn.mevo import BaselineSoftmaxNllLoss, get_data
+from fairscale.fair_dev.testing.testing import skip_if_no_cuda
 
 
 @pytest.fixture(scope="session", params=[torch.float16, torch.float32])
diff --git a/tests/experimental/nn/test_multiprocess_pipe.py b/tests/experimental/nn/test_multiprocess_pipe.py
index 8ef6b94..ee17a7d 100644
--- a/tests/experimental/nn/test_multiprocess_pipe.py
+++ b/tests/experimental/nn/test_multiprocess_pipe.py
@@ -20,8 +20,8 @@ import torch.distributed.rpc as rpc
 import torch.multiprocessing as mp
 import torch.nn as nn
 
-from fair_dev.testing.testing import skip_due_to_flakyness, skip_if_single_gpu
 from fairscale.experimental.nn.distributed_pipeline import DistributedLoss, DistributedPipeline, PipelineModulesGraph
+from fairscale.fair_dev.testing.testing import skip_due_to_flakyness, skip_if_single_gpu
 from fairscale.internal import torch_version
 
 pytestmark = pytest.mark.skipif(
diff --git a/tests/experimental/nn/test_offload.py b/tests/experimental/nn/test_offload.py
index 2f73522..2cf339d 100644
--- a/tests/experimental/nn/test_offload.py
+++ b/tests/experimental/nn/test_offload.py
@@ -14,8 +14,8 @@ import numpy as np
 import pytest
 import torch
 
-from fair_dev.testing.testing import skip_if_no_cuda
 from fairscale.experimental.nn.offload import OffloadModel
+from fairscale.fair_dev.testing.testing import skip_if_no_cuda
 from fairscale.internal import torch_version
 
 if torch_version() >= (1, 8, 0):
diff --git a/tests/experimental/tooling/test_layer_memory_tracker.py b/tests/experimental/tooling/test_layer_memory_tracker.py
index 3c7cd6a..c930077 100644
--- a/tests/experimental/tooling/test_layer_memory_tracker.py
+++ b/tests/experimental/tooling/test_layer_memory_tracker.py
@@ -10,12 +10,12 @@ import torch.multiprocessing as mp
 import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel
 
-from fair_dev.testing.testing import GPT2, dist_init, skip_if_no_cuda, skip_if_single_gpu, temp_files_ctx
 from fairscale.experimental.tooling.layer_memory_tracker import (
     LayerwiseMemoryTracker,
     ProcessGroupTracker,
     find_best_reset_points,
 )
+from fairscale.fair_dev.testing.testing import GPT2, dist_init, skip_if_no_cuda, skip_if_single_gpu, temp_files_ctx
 from fairscale.nn import FullyShardedDataParallel
 
 
diff --git a/tests/experimental/wgit/test_sha1_store.py b/tests/experimental/wgit/test_sha1_store.py
index 63766fb..976e866 100644
--- a/tests/experimental/wgit/test_sha1_store.py
+++ b/tests/experimental/wgit/test_sha1_store.py
@@ -11,8 +11,8 @@ import pytest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import objects_are_equal
 from fairscale.experimental.wgit.sha1_store import SHA1_Store
+from fairscale.fair_dev.testing.testing import objects_are_equal
 
 # Get the absolute path of the parent at the beginning before any os.chdir(),
 # so that we can proper clean it up at any CWD.
diff --git a/tests/experimental/wgit/test_signal_sparsity.py b/tests/experimental/wgit/test_signal_sparsity.py
index a4db52b..4a32153 100644
--- a/tests/experimental/wgit/test_signal_sparsity.py
+++ b/tests/experimental/wgit/test_signal_sparsity.py
@@ -6,8 +6,8 @@
 import pytest
 import torch
 
-from fair_dev.testing.testing import objects_are_equal
 from fairscale.experimental.wgit.signal_sparsity import SignalSparsity, random_sparse_mask
+from fairscale.fair_dev.testing.testing import objects_are_equal
 
 # Our own tolerance
 ATOL = 1e-6
diff --git a/tests/experimental/wgit/test_signal_sparsity_profiling.py b/tests/experimental/wgit/test_signal_sparsity_profiling.py
index a3be08c..f3ed00f 100644
--- a/tests/experimental/wgit/test_signal_sparsity_profiling.py
+++ b/tests/experimental/wgit/test_signal_sparsity_profiling.py
@@ -8,8 +8,8 @@ import time
 import pytest
 import torch
 
-from fair_dev.testing.testing import objects_are_equal, skip_if_no_cuda
 from fairscale.experimental.wgit.signal_sparsity_profiling import EnergyConcentrationProfile as ECP
+from fairscale.fair_dev.testing.testing import objects_are_equal, skip_if_no_cuda
 
 # Our own tolerance
 ATOL = 1e-6
diff --git a/tests/nn/checkpoint/test_checkpoint_activations.py b/tests/nn/checkpoint/test_checkpoint_activations.py
index e1747f2..ef04612 100644
--- a/tests/nn/checkpoint/test_checkpoint_activations.py
+++ b/tests/nn/checkpoint/test_checkpoint_activations.py
@@ -10,7 +10,7 @@ import torch
 import torch.nn as nn
 from torch.utils.checkpoint import checkpoint as torch_checkpoint_wrapper
 
-from fair_dev.testing.testing import skip_if_no_cuda
+from fairscale.fair_dev.testing.testing import skip_if_no_cuda
 from fairscale.internal import torch_version
 from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper, disable_checkpointing
 from fairscale.nn.misc import FlattenParamsWrapper
diff --git a/tests/nn/checkpoint/test_checkpoint_activations_norm.py b/tests/nn/checkpoint/test_checkpoint_activations_norm.py
index 09ee108..504c9c5 100644
--- a/tests/nn/checkpoint/test_checkpoint_activations_norm.py
+++ b/tests/nn/checkpoint/test_checkpoint_activations_norm.py
@@ -14,7 +14,7 @@ import torch
 from torch.nn import BatchNorm2d, LayerNorm, Linear, Sequential
 from torch.optim import SGD
 
-from fair_dev.testing.testing import objects_are_equal
+from fairscale.fair_dev.testing.testing import objects_are_equal
 from fairscale.internal import torch_version
 from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper
 
diff --git a/tests/nn/data_parallel/test_fsdp.py b/tests/nn/data_parallel/test_fsdp.py
index c3d0275..e67c65f 100644
--- a/tests/nn/data_parallel/test_fsdp.py
+++ b/tests/nn/data_parallel/test_fsdp.py
@@ -18,7 +18,7 @@ import torch
 from torch import nn
 import torch.distributed
 
-from fair_dev.testing.testing import (
+from fairscale.fair_dev.testing.testing import (
     DeviceAndTypeCheckModule,
     DummyProcessGroup,
     dist_init,
diff --git a/tests/nn/data_parallel/test_fsdp_freezing_weights.py b/tests/nn/data_parallel/test_fsdp_freezing_weights.py
index c06a621..36cbdd2 100644
--- a/tests/nn/data_parallel/test_fsdp_freezing_weights.py
+++ b/tests/nn/data_parallel/test_fsdp_freezing_weights.py
@@ -21,7 +21,7 @@ import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel
 import torch.optim as optim
 
-from fair_dev.testing.testing import dist_init, objects_are_equal, rmf, skip_if_single_gpu, teardown
+from fairscale.fair_dev.testing.testing import dist_init, objects_are_equal, rmf, skip_if_single_gpu, teardown
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 
 
diff --git a/tests/nn/data_parallel/test_fsdp_grad_acc.py b/tests/nn/data_parallel/test_fsdp_grad_acc.py
index 1b67425..49a6375 100644
--- a/tests/nn/data_parallel/test_fsdp_grad_acc.py
+++ b/tests/nn/data_parallel/test_fsdp_grad_acc.py
@@ -12,7 +12,7 @@ from unittest.mock import patch
 from parameterized import parameterized
 import torch
 
-from fair_dev.testing.testing import DummyProcessGroup, make_cudnn_deterministic, objects_are_equal
+from fairscale.fair_dev.testing.testing import DummyProcessGroup, make_cudnn_deterministic, objects_are_equal
 from fairscale.nn.data_parallel import FullyShardedDataParallel
 
 from .test_fsdp import DistributedTest, NestedWrappedModule, rename_test, spawn_and_init
diff --git a/tests/nn/data_parallel/test_fsdp_hf_transformer_eval.py b/tests/nn/data_parallel/test_fsdp_hf_transformer_eval.py
index 4c0876b..30956f0 100644
--- a/tests/nn/data_parallel/test_fsdp_hf_transformer_eval.py
+++ b/tests/nn/data_parallel/test_fsdp_hf_transformer_eval.py
@@ -6,7 +6,7 @@ import unittest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import dist_init
+from fairscale.fair_dev.testing.testing import dist_init
 from fairscale.nn import FullyShardedDataParallel as FSDP
 from fairscale.nn import auto_wrap, enable_wrap
 
diff --git a/tests/nn/data_parallel/test_fsdp_input.py b/tests/nn/data_parallel/test_fsdp_input.py
index e437d5f..f1824c8 100644
--- a/tests/nn/data_parallel/test_fsdp_input.py
+++ b/tests/nn/data_parallel/test_fsdp_input.py
@@ -16,7 +16,7 @@ import torch
 from torch.nn import Linear, Module
 from torch.optim import SGD
 
-from fair_dev.testing.testing import dist_init, rmf, skip_if_no_cuda, teardown
+from fairscale.fair_dev.testing.testing import dist_init, rmf, skip_if_no_cuda, teardown
 from fairscale.internal import torch_version
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 from fairscale.nn.data_parallel import TrainingState
diff --git a/tests/nn/data_parallel/test_fsdp_memory.py b/tests/nn/data_parallel/test_fsdp_memory.py
index 583f8f0..b465e25 100644
--- a/tests/nn/data_parallel/test_fsdp_memory.py
+++ b/tests/nn/data_parallel/test_fsdp_memory.py
@@ -18,7 +18,7 @@ import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel
 import torch.optim as optim
 
-from fair_dev.testing.testing import dist_init, dump_all_tensors, skip_if_single_gpu, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import dist_init, dump_all_tensors, skip_if_single_gpu, teardown, temp_files_ctx
 from fairscale.internal import torch_version
 from fairscale.internal.parallel import get_process_group_cached
 from fairscale.nn import checkpoint_wrapper
diff --git a/tests/nn/data_parallel/test_fsdp_metadata.py b/tests/nn/data_parallel/test_fsdp_metadata.py
index 508e255..c28ab81 100644
--- a/tests/nn/data_parallel/test_fsdp_metadata.py
+++ b/tests/nn/data_parallel/test_fsdp_metadata.py
@@ -14,7 +14,7 @@ import torch.multiprocessing as mp
 import torch.nn as nn
 from torch.optim import Adam
 
-from fair_dev.testing.testing import in_temporary_directory, skip_if_single_gpu, temp_files_ctx
+from fairscale.fair_dev.testing.testing import in_temporary_directory, skip_if_single_gpu, temp_files_ctx
 from fairscale.nn import FullyShardedDataParallel
 from tests.nn.data_parallel.test_fsdp import DistributedTest, MixtureOfExperts, rename_test, spawn_and_init
 
diff --git a/tests/nn/data_parallel/test_fsdp_multiple_forward.py b/tests/nn/data_parallel/test_fsdp_multiple_forward.py
index 6502179..de3b5ec 100644
--- a/tests/nn/data_parallel/test_fsdp_multiple_forward.py
+++ b/tests/nn/data_parallel/test_fsdp_multiple_forward.py
@@ -17,7 +17,7 @@ import torch.multiprocessing as mp
 from torch.nn import Linear, Module
 from torch.optim import SGD
 
-from fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown
+from fairscale.fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown
 from fairscale.internal import torch_version
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 from fairscale.nn.data_parallel import TrainingState
diff --git a/tests/nn/data_parallel/test_fsdp_multiple_forward_checkpoint.py b/tests/nn/data_parallel/test_fsdp_multiple_forward_checkpoint.py
index f757543..1f30f3c 100644
--- a/tests/nn/data_parallel/test_fsdp_multiple_forward_checkpoint.py
+++ b/tests/nn/data_parallel/test_fsdp_multiple_forward_checkpoint.py
@@ -20,7 +20,7 @@ import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel
 import torch.optim as optim
 
-from fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown, temp_files_ctx
 from fairscale.internal import torch_version
 from fairscale.nn import checkpoint_wrapper
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
diff --git a/tests/nn/data_parallel/test_fsdp_multiple_wrapping.py b/tests/nn/data_parallel/test_fsdp_multiple_wrapping.py
index 17eaace..7cfecb2 100644
--- a/tests/nn/data_parallel/test_fsdp_multiple_wrapping.py
+++ b/tests/nn/data_parallel/test_fsdp_multiple_wrapping.py
@@ -17,7 +17,7 @@ import torch.multiprocessing as mp
 from torch.nn import Linear, Module, Sequential
 from torch.optim import SGD
 
-from fair_dev.testing.testing import dist_init, skip_if_no_cuda, teardown
+from fairscale.fair_dev.testing.testing import dist_init, skip_if_no_cuda, teardown
 from fairscale.internal import torch_version
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 from fairscale.nn.data_parallel import TrainingState
diff --git a/tests/nn/data_parallel/test_fsdp_offload.py b/tests/nn/data_parallel/test_fsdp_offload.py
index 0ae6eb4..a5b83b0 100644
--- a/tests/nn/data_parallel/test_fsdp_offload.py
+++ b/tests/nn/data_parallel/test_fsdp_offload.py
@@ -25,7 +25,7 @@ except ImportError as ie:
     pytestmark = pytest.mark.skipif(True, reason=ie.msg)
     pass
 
-from fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
 from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper
 from fairscale.nn.data_parallel import FullyShardedDataParallel, OffloadConfig, TrainingState
 
diff --git a/tests/nn/data_parallel/test_fsdp_optimizer_utils.py b/tests/nn/data_parallel/test_fsdp_optimizer_utils.py
index 8095c60..ad17612 100644
--- a/tests/nn/data_parallel/test_fsdp_optimizer_utils.py
+++ b/tests/nn/data_parallel/test_fsdp_optimizer_utils.py
@@ -12,7 +12,7 @@ import torch
 from torch import nn
 from torch.optim import SGD, Adadelta, Adam  # type: ignore
 
-from fair_dev.testing.testing import dist_init, objects_are_equal, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import dist_init, objects_are_equal, spawn_for_all_world_sizes
 from fairscale.internal.params import recursive_copy_to_device
 from fairscale.nn.data_parallel import FullyShardedDataParallel, get_fsdp_instances
 from fairscale.nn.data_parallel.fsdp_optim_utils import is_singleton_tensor
diff --git a/tests/nn/data_parallel/test_fsdp_overlap.py b/tests/nn/data_parallel/test_fsdp_overlap.py
index e1e5846..ae07088 100644
--- a/tests/nn/data_parallel/test_fsdp_overlap.py
+++ b/tests/nn/data_parallel/test_fsdp_overlap.py
@@ -19,7 +19,13 @@ from torch.cuda import Event
 import torch.multiprocessing as mp
 import torch.nn as nn
 
-from fair_dev.testing.testing import dist_init, get_cycles_per_ms, skip_if_single_gpu, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import (
+    dist_init,
+    get_cycles_per_ms,
+    skip_if_single_gpu,
+    teardown,
+    temp_files_ctx,
+)
 from fairscale.internal import torch_version
 from fairscale.nn import enable_wrap, wrap
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
diff --git a/tests/nn/data_parallel/test_fsdp_pre_backward_hook.py b/tests/nn/data_parallel/test_fsdp_pre_backward_hook.py
index 756cdd1..aa04361 100644
--- a/tests/nn/data_parallel/test_fsdp_pre_backward_hook.py
+++ b/tests/nn/data_parallel/test_fsdp_pre_backward_hook.py
@@ -13,7 +13,7 @@ import pytest
 import torch
 from torch.nn import Linear, Module
 
-from fair_dev.testing.testing import dist_init, skip_if_no_cuda, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import dist_init, skip_if_no_cuda, teardown, temp_files_ctx
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 
 
diff --git a/tests/nn/data_parallel/test_fsdp_regnet.py b/tests/nn/data_parallel/test_fsdp_regnet.py
index 004a3d8..e402082 100644
--- a/tests/nn/data_parallel/test_fsdp_regnet.py
+++ b/tests/nn/data_parallel/test_fsdp_regnet.py
@@ -33,7 +33,7 @@ from torch.nn import (
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.optim import SGD
 
-from fair_dev.testing.testing import (
+from fairscale.fair_dev.testing.testing import (
     dist_init,
     objects_are_equal,
     rmf,
diff --git a/tests/nn/data_parallel/test_fsdp_shared_weights.py b/tests/nn/data_parallel/test_fsdp_shared_weights.py
index e6711f9..05ff220 100644
--- a/tests/nn/data_parallel/test_fsdp_shared_weights.py
+++ b/tests/nn/data_parallel/test_fsdp_shared_weights.py
@@ -17,7 +17,13 @@ import torch.multiprocessing as mp
 from torch.nn import Linear, Module
 from torch.optim import SGD
 
-from fair_dev.testing.testing import dist_init, objects_are_equal, skip_if_single_gpu, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import (
+    dist_init,
+    objects_are_equal,
+    skip_if_single_gpu,
+    teardown,
+    temp_files_ctx,
+)
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 
 
diff --git a/tests/nn/data_parallel/test_fsdp_shared_weights_mevo.py b/tests/nn/data_parallel/test_fsdp_shared_weights_mevo.py
index 7807334..8754ae1 100644
--- a/tests/nn/data_parallel/test_fsdp_shared_weights_mevo.py
+++ b/tests/nn/data_parallel/test_fsdp_shared_weights_mevo.py
@@ -17,7 +17,8 @@ from torch import nn
 import torch.multiprocessing as mp
 from torch.optim import SGD
 
-from fair_dev.testing.testing import (
+from fairscale.experimental.nn import MEVO
+from fairscale.fair_dev.testing.testing import (
     dist_init,
     in_circle_ci,
     objects_are_equal,
@@ -25,7 +26,6 @@ from fair_dev.testing.testing import (
     teardown,
     temp_files_ctx,
 )
-from fairscale.experimental.nn import MEVO
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 
 VOCAB = 4
diff --git a/tests/nn/data_parallel/test_fsdp_state_dict.py b/tests/nn/data_parallel/test_fsdp_state_dict.py
index d41bde6..0270940 100644
--- a/tests/nn/data_parallel/test_fsdp_state_dict.py
+++ b/tests/nn/data_parallel/test_fsdp_state_dict.py
@@ -11,7 +11,7 @@ import pytest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import dist_init, objects_are_equal, skip_if_cuda, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import dist_init, objects_are_equal, skip_if_cuda, teardown, temp_files_ctx
 from fairscale.internal import torch_version
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 
diff --git a/tests/nn/data_parallel/test_fsdp_uneven.py b/tests/nn/data_parallel/test_fsdp_uneven.py
index 0a6bb38..8cadd2a 100644
--- a/tests/nn/data_parallel/test_fsdp_uneven.py
+++ b/tests/nn/data_parallel/test_fsdp_uneven.py
@@ -18,7 +18,7 @@ import torch.multiprocessing as mp
 from torch.nn import Linear, Sequential
 from torch.optim import SGD
 
-from fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown
+from fairscale.fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown
 from fairscale.internal import torch_version
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 from fairscale.nn.data_parallel.fully_sharded_data_parallel import TrainingState
diff --git a/tests/nn/data_parallel/test_fsdp_with_checkpoint_wrapper.py b/tests/nn/data_parallel/test_fsdp_with_checkpoint_wrapper.py
index 2ba592f..81247ec 100644
--- a/tests/nn/data_parallel/test_fsdp_with_checkpoint_wrapper.py
+++ b/tests/nn/data_parallel/test_fsdp_with_checkpoint_wrapper.py
@@ -13,7 +13,7 @@ from torch import nn
 import torch.distributed
 import torch.multiprocessing as mp
 
-from fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown, temp_files_ctx
+from fairscale.fair_dev.testing.testing import dist_init, skip_if_single_gpu, teardown, temp_files_ctx
 from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 
diff --git a/tests/nn/data_parallel/test_sharded_ddp_features.py b/tests/nn/data_parallel/test_sharded_ddp_features.py
index 97c31c6..df491b6 100644
--- a/tests/nn/data_parallel/test_sharded_ddp_features.py
+++ b/tests/nn/data_parallel/test_sharded_ddp_features.py
@@ -16,7 +16,7 @@ import torch.distributed as dist
 import torch.multiprocessing as mp
 from torch.nn import Linear, Sequential
 
-from fair_dev.testing.testing import (
+from fairscale.fair_dev.testing.testing import (
     GPT2,
     SGDWithPausingCompute,
     available_devices,
diff --git a/tests/nn/data_parallel/test_sharded_ddp_pytorch_parity.py b/tests/nn/data_parallel/test_sharded_ddp_pytorch_parity.py
index d9cb886..5bbb56b 100644
--- a/tests/nn/data_parallel/test_sharded_ddp_pytorch_parity.py
+++ b/tests/nn/data_parallel/test_sharded_ddp_pytorch_parity.py
@@ -19,7 +19,12 @@ import torch.multiprocessing as mp
 from torch.nn import Linear, Sequential
 from torch.nn.parallel import DistributedDataParallel as DDP
 
-from fair_dev.testing.testing import check_same_model_params, skip_if_no_cuda, skip_if_single_gpu, temp_files_ctx
+from fairscale.fair_dev.testing.testing import (
+    check_same_model_params,
+    skip_if_no_cuda,
+    skip_if_single_gpu,
+    temp_files_ctx,
+)
 from fairscale.internal import torch_version
 from fairscale.nn.data_parallel import ShardedDataParallel
 from fairscale.optim import OSS
diff --git a/tests/nn/misc/test_flatten_params_wrapper.py b/tests/nn/misc/test_flatten_params_wrapper.py
index ca1971d..1f919d5 100644
--- a/tests/nn/misc/test_flatten_params_wrapper.py
+++ b/tests/nn/misc/test_flatten_params_wrapper.py
@@ -10,7 +10,7 @@ import unittest
 
 import torch
 
-from fair_dev.testing.testing import objects_are_equal
+from fairscale.fair_dev.testing.testing import objects_are_equal
 from fairscale.nn import FlattenParamsWrapper
 
 
diff --git a/tests/nn/model_parallel/test_cross_entropy.py b/tests/nn/model_parallel/test_cross_entropy.py
index 9631694..6537dc5 100644
--- a/tests/nn/model_parallel/test_cross_entropy.py
+++ b/tests/nn/model_parallel/test_cross_entropy.py
@@ -23,7 +23,7 @@
 import torch
 import torch.nn.functional as F
 
-from fair_dev.testing.testing import IdentityLayer, dist_init, set_random_seed, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import IdentityLayer, dist_init, set_random_seed, spawn_for_all_world_sizes
 from fairscale.nn.model_parallel import initialize as mpu
 from fairscale.nn.model_parallel.cross_entropy import vocab_parallel_cross_entropy
 from fairscale.nn.model_parallel.mappings import scatter_to_model_parallel_region
diff --git a/tests/nn/model_parallel/test_initialize.py b/tests/nn/model_parallel/test_initialize.py
index c983750..131d0a9 100644
--- a/tests/nn/model_parallel/test_initialize.py
+++ b/tests/nn/model_parallel/test_initialize.py
@@ -22,7 +22,7 @@
 
 import torch
 
-from fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
 from fairscale.nn.model_parallel import initialize as mpu
 
 
diff --git a/tests/nn/model_parallel/test_layers.py b/tests/nn/model_parallel/test_layers.py
index 609dda9..2c7f4ed 100644
--- a/tests/nn/model_parallel/test_layers.py
+++ b/tests/nn/model_parallel/test_layers.py
@@ -24,7 +24,7 @@ import torch
 import torch.nn.init as init
 from torch.nn.parameter import Parameter
 
-from fair_dev.testing.testing import dist_init, set_random_seed, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import dist_init, set_random_seed, spawn_for_all_world_sizes
 from fairscale.nn.model_parallel import initialize as mpu
 from fairscale.nn.model_parallel import layers
 
diff --git a/tests/nn/model_parallel/test_random.py b/tests/nn/model_parallel/test_random.py
index dbe9487..03574e8 100644
--- a/tests/nn/model_parallel/test_random.py
+++ b/tests/nn/model_parallel/test_random.py
@@ -21,7 +21,7 @@
 
 import torch
 
-from fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
 from fairscale.nn.model_parallel import initialize as mpu
 from fairscale.nn.model_parallel import random
 from fairscale.nn.model_parallel.random import get_cuda_rng_tracker, model_parallel_cuda_manual_seed
diff --git a/tests/nn/moe/test_moe_layer.py b/tests/nn/moe/test_moe_layer.py
index c87af15..73db196 100644
--- a/tests/nn/moe/test_moe_layer.py
+++ b/tests/nn/moe/test_moe_layer.py
@@ -11,7 +11,7 @@ import torch
 import torch.distributed as dist
 import torch.multiprocessing as mp
 
-from fair_dev.testing.testing import make_cudnn_deterministic
+from fairscale.fair_dev.testing.testing import make_cudnn_deterministic
 from fairscale.internal import torch_version
 from fairscale.nn import MOELayer, Top2Gate
 
diff --git a/tests/nn/pipe/skip/test_gpipe.py b/tests/nn/pipe/skip/test_gpipe.py
index b666e32..54e4548 100644
--- a/tests/nn/pipe/skip/test_gpipe.py
+++ b/tests/nn/pipe/skip/test_gpipe.py
@@ -21,7 +21,7 @@ import pytest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import skip_if_single_gpu
+from fairscale.fair_dev.testing.testing import skip_if_single_gpu
 from fairscale.nn.pipe import Pipe
 from fairscale.nn.pipe.skip import pop, skippable, stash
 from fairscale.nn.pipe.skip.portal import PortalBlue, PortalCopy, PortalOrange
diff --git a/tests/nn/pipe/test_bugs.py b/tests/nn/pipe/test_bugs.py
index 83657f2..7a48eb7 100644
--- a/tests/nn/pipe/test_bugs.py
+++ b/tests/nn/pipe/test_bugs.py
@@ -22,7 +22,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 
-from fair_dev.testing.testing import skip_if_single_gpu
+from fairscale.fair_dev.testing.testing import skip_if_single_gpu
 from fairscale.nn.pipe import Pipe
 
 
diff --git a/tests/nn/pipe/test_checkpoint_ddp.py b/tests/nn/pipe/test_checkpoint_ddp.py
index 38b2e99..254e7a3 100644
--- a/tests/nn/pipe/test_checkpoint_ddp.py
+++ b/tests/nn/pipe/test_checkpoint_ddp.py
@@ -20,7 +20,7 @@ from torch.nn import Linear, Sequential
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.utils.checkpoint import checkpoint as torch_checkpoint
 
-from fair_dev.testing.testing import skip_if_no_cuda, skip_if_single_gpu
+from fairscale.fair_dev.testing.testing import skip_if_no_cuda, skip_if_single_gpu
 from fairscale.nn.pipe.checkpoint import Checkpointing, Function, TensorOrTensors
 from fairscale.nn.pipe.microbatch import Batch
 
diff --git a/tests/nn/pipe/test_parity.py b/tests/nn/pipe/test_parity.py
index d1edee0..978967c 100644
--- a/tests/nn/pipe/test_parity.py
+++ b/tests/nn/pipe/test_parity.py
@@ -14,7 +14,7 @@ import numpy as np
 import pytest
 import torch
 
-from fair_dev.testing.testing import skip_if_single_gpu
+from fairscale.fair_dev.testing.testing import skip_if_single_gpu
 from fairscale.nn import Pipe
 
 
diff --git a/tests/nn/pipe_process/test_bugs.py b/tests/nn/pipe_process/test_bugs.py
index dc78c01..d970cf1 100644
--- a/tests/nn/pipe_process/test_bugs.py
+++ b/tests/nn/pipe_process/test_bugs.py
@@ -22,7 +22,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 
-from fair_dev.testing.testing import get_worker_map, torch_spawn
+from fairscale.fair_dev.testing.testing import get_worker_map, torch_spawn
 from fairscale.nn.pipe import AsyncPipe
 
 
diff --git a/tests/nn/pipe_process/test_inplace.py b/tests/nn/pipe_process/test_inplace.py
index dbbf47f..6502426 100644
--- a/tests/nn/pipe_process/test_inplace.py
+++ b/tests/nn/pipe_process/test_inplace.py
@@ -21,7 +21,7 @@ import pytest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import get_worker_map, torch_spawn
+from fairscale.fair_dev.testing.testing import get_worker_map, torch_spawn
 from fairscale.nn.pipe import AsyncPipe
 
 
diff --git a/tests/nn/pipe_process/test_pipe.py b/tests/nn/pipe_process/test_pipe.py
index a99ba15..19de13a 100644
--- a/tests/nn/pipe_process/test_pipe.py
+++ b/tests/nn/pipe_process/test_pipe.py
@@ -26,7 +26,7 @@ import pytest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import get_worker_map, torch_spawn
+from fairscale.fair_dev.testing.testing import get_worker_map, torch_spawn
 from fairscale.internal import torch_version
 from fairscale.nn.model_parallel.initialize import get_pipeline_parallel_group
 from fairscale.nn.pipe import AsyncPipe
diff --git a/tests/nn/pipe_process/test_rpc.py b/tests/nn/pipe_process/test_rpc.py
index 777268d..97f0b0e 100644
--- a/tests/nn/pipe_process/test_rpc.py
+++ b/tests/nn/pipe_process/test_rpc.py
@@ -6,7 +6,7 @@ import torch
 from torch import nn
 from torch.distributed import rpc
 
-from fair_dev.testing.testing import get_worker_map, torch_spawn
+from fairscale.fair_dev.testing.testing import get_worker_map, torch_spawn
 from fairscale.internal import torch_version
 from fairscale.nn.model_parallel.initialize import get_pipeline_parallel_group
 from fairscale.nn.pipe import PipeRPCWrapper
diff --git a/tests/nn/pipe_process/test_transparency.py b/tests/nn/pipe_process/test_transparency.py
index da8cf42..f517dcd 100644
--- a/tests/nn/pipe_process/test_transparency.py
+++ b/tests/nn/pipe_process/test_transparency.py
@@ -21,7 +21,7 @@ import pytest
 import torch
 from torch import nn
 
-from fair_dev.testing.testing import get_worker_map, set_random_seed, torch_spawn
+from fairscale.fair_dev.testing.testing import get_worker_map, set_random_seed, torch_spawn
 from fairscale.nn.pipe import AsyncPipe
 
 
diff --git a/tests/nn/wrap/test_wrap.py b/tests/nn/wrap/test_wrap.py
index 93cb926..c802662 100644
--- a/tests/nn/wrap/test_wrap.py
+++ b/tests/nn/wrap/test_wrap.py
@@ -12,7 +12,7 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from fair_dev.testing.testing import DummyProcessGroup
+from fairscale.fair_dev.testing.testing import DummyProcessGroup
 from fairscale.nn import FullyShardedDataParallel as FSDP
 from fairscale.nn import auto_wrap, default_auto_wrap_policy, enable_wrap, wrap
 
diff --git a/tests/optim/test_ddp_adascale.py b/tests/optim/test_ddp_adascale.py
index 1b1aab6..5b7a8d4 100644
--- a/tests/optim/test_ddp_adascale.py
+++ b/tests/optim/test_ddp_adascale.py
@@ -33,8 +33,8 @@ from torch.nn import Linear
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.optim import SGD
 
-from fair_dev.testing.golden_testing_data import adascale_test_data
-from fair_dev.testing.testing import skip_if_single_gpu
+from fairscale.fair_dev.testing.golden_testing_data import adascale_test_data
+from fairscale.fair_dev.testing.testing import skip_if_single_gpu
 from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP
 from fairscale.nn.data_parallel import ShardedDataParallel as SDP
 from fairscale.optim import OSS, AdaScale
diff --git a/tests/optim/test_layerwise_gradient_scaler.py b/tests/optim/test_layerwise_gradient_scaler.py
index d8c7442..f968113 100644
--- a/tests/optim/test_layerwise_gradient_scaler.py
+++ b/tests/optim/test_layerwise_gradient_scaler.py
@@ -17,8 +17,8 @@ from torch.utils.data import DataLoader
 import torchvision
 import torchvision.transforms as transforms
 
-from fair_dev.common_paths import DATASET_CACHE_ROOT
-from fair_dev.testing.testing import skip_a_test_if_in_CI
+from fairscale.fair_dev.common_paths import DATASET_CACHE_ROOT
+from fairscale.fair_dev.testing.testing import skip_a_test_if_in_CI
 from fairscale.optim.layerwise_gradient_scaler import LayerwiseGradientScaler
 
 
diff --git a/tests/optim/test_oss.py b/tests/optim/test_oss.py
index 930f700..179e493 100644
--- a/tests/optim/test_oss.py
+++ b/tests/optim/test_oss.py
@@ -21,7 +21,7 @@ import torch.distributed as dist
 import torch.multiprocessing as mp
 from torch.nn.parallel import DistributedDataParallel as DDP
 
-from fair_dev.testing.testing import (
+from fairscale.fair_dev.testing.testing import (
     check_same_model_params,
     check_same_models_across_ranks,
     skip_if_no_cuda,
diff --git a/tests/optim/test_oss_adascale.py b/tests/optim/test_oss_adascale.py
index 41e9a1b..8310690 100644
--- a/tests/optim/test_oss_adascale.py
+++ b/tests/optim/test_oss_adascale.py
@@ -22,8 +22,8 @@ from torch.nn import Linear, Sequential
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.optim import SGD
 
-from fair_dev.testing.golden_testing_data import adascale_test_data
-from fair_dev.testing.testing import skip_if_single_gpu
+from fairscale.fair_dev.testing.golden_testing_data import adascale_test_data
+from fairscale.fair_dev.testing.testing import skip_if_single_gpu
 from fairscale.optim import OSS, AdaScale, AdaScaleWrapper
 
 
diff --git a/tests/optim/test_single_node_adascale.py b/tests/optim/test_single_node_adascale.py
index af6d8df..f8877b3 100644
--- a/tests/optim/test_single_node_adascale.py
+++ b/tests/optim/test_single_node_adascale.py
@@ -19,9 +19,9 @@ from torch.nn import Linear, Sequential
 from torch.optim import SGD
 from torch.optim.lr_scheduler import LambdaLR
 
-from fair_dev.testing.golden_testing_data import adascale_test_data
-from fair_dev.testing.testing import make_cudnn_deterministic, skip_if_no_cuda
-from fair_dev.testing.testing_memory import find_tensor_by_shape
+from fairscale.fair_dev.testing.golden_testing_data import adascale_test_data
+from fairscale.fair_dev.testing.testing import make_cudnn_deterministic, skip_if_no_cuda
+from fairscale.fair_dev.testing.testing_memory import find_tensor_by_shape
 from fairscale.optim import AdaScale
 
 
diff --git a/tests/utils/test_reduce_scatter_bucketer.py b/tests/utils/test_reduce_scatter_bucketer.py
index e559b8d..2d03c07 100644
--- a/tests/utils/test_reduce_scatter_bucketer.py
+++ b/tests/utils/test_reduce_scatter_bucketer.py
@@ -12,7 +12,7 @@ from unittest import mock
 from parameterized import parameterized
 import torch
 
-from fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
+from fairscale.fair_dev.testing.testing import dist_init, spawn_for_all_world_sizes
 from fairscale.internal import torch_version
 from fairscale.internal.reduce_scatter_bucketer import ReduceScatterBucketer
 
-- 
2.38.1.windows.1

